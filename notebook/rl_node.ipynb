{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for node deployment\n",
    "\n",
    "### Environment\n",
    "The example is modifed from the Taxi Problem in \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by  Tom Dietterich (2000), Journal of Artificial Intelligence Research.\n",
    "\n",
    "<!--img style=\"float: right;\" src=\"images/DuckieNav-v1.png\"  width=\"240\" height=\"240\"-->\n",
    "\n",
    "\n",
    "```\n",
    "MAP = [\n",
    "    \"+-----------------+\",\n",
    "    \"| | | : : : : : : |\",\n",
    "    \"| | | | | | | | | |\",\n",
    "    \"| | : | | | | | | |\",\n",
    "    \"| : | | | : : : : |\",\n",
    "    \"| | | | | | | | | |\",\n",
    "    \"| : : : : : : | | |\",\n",
    "    \"| | | | | | | | | |\",\n",
    "    \"| | | : : | | : : |\",\n",
    "    \"| | | | | | | | | |\",\n",
    "    \"| : : : : : : | | |\",\n",
    "    \"| | | | | | | | | |\",\n",
    "    \"| : : : : : : | | |\",\n",
    "    \"| | | | | | | | | |\",\n",
    "    \"| : : : : : : : : |\",\n",
    "    \"+-----------------+\",\n",
    "]\n",
    "```\n",
    "\n",
    "We consider shows a 14 by 9 grid world, except the \"service area.\" The taxi problem is episodic, and in each episode the initial location is located at one of the 4 specially designated locations. \n",
    "(1,1), (3,3), (5,5), (7,7). The robot(agent) starts in a given location and must go to some location and deploy 2 nodes with maximum coverage. The episode ends when 2 nodes is deployed.\n",
    "\n",
    "Adapted from https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize ENV\n",
    "\n",
    "### Installation\n",
    "```\n",
    "$ cd ~/gym-duckienav\n",
    "$ git pull\n",
    "$ pip install -e . # you may need sudo depending on your setup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pickle\n",
    "import gym_duckienav\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"DeployNav-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many 'states' in observation_space: \n",
    "There are 16003008 states from: (14 (rows) x 9 (columns) ^ 3) x 4 (initial location) x 2 (deploy number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16003008"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### action_space: \n",
    "There are 5 possible actions.\n",
    "* down (0), up (1), right (2), left (3), deploy (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save env to avoid initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filehandler = open('comm_nav_14_9_16003008.pkl', 'wb') \n",
    "#pickle.dump(env, filehandler)\n",
    "#filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. States\n",
    "\n",
    "Resets the state of the environment and returns an initial observation (state).\n",
    "\n",
    "The current state is from :\n",
    "* initial location\n",
    "* how many node is dropped\n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: 4160782\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "[1, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[44m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| : : : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "print (\"Current state: \" + str(env.s))\n",
    "for p in env.decode(env.s): print (p)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat previous cell for a few times.\n",
    "\n",
    "In taxi problem, the colors mean:\n",
    "* blue: passenger's current position\n",
    "* magenta: destination\n",
    "* yellow: empty taxi\n",
    "* green: full taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Actions\n",
    "\n",
    "Remember that the taxi agent can perform the following actions:\n",
    "* 0: \"South\", \n",
    "* 1: \"North\", \n",
    "* 2: \"East\", \n",
    "* 3: \"West\", \n",
    "* 4: \"Pickup\", \n",
    "* 5: \"Dropoff\"\n",
    "\n",
    "Let's set the state to 124.\n",
    "Let the taxi agent perform some actions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `step()`\n",
    "\n",
    "Run one timestep of the environment's dynamics. \n",
    "It returns a tuple (observation, reward, done, info)\n",
    "* observation (object): agent's observation of the current environment\n",
    "* reward (float) : amount of reward returned after previous action\n",
    "* done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "* info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "\n",
    "Essentially the empty taxi is supposed to: \n",
    "* move toward the blue letter, \n",
    "* pickup the passenger (now the taxi is green), \n",
    "* drive to the magenta letter, and \n",
    "* drop the passenger (the taxi is yellow again).\n",
    "\n",
    "It is obvious that we should start with moving \"East\" env.step(2). Index 2 is for moving \"East\"\n",
    "We will do the followings:\n",
    "* Perform \"Pickup\" step(4) (although the passenger is not here)\n",
    "* Perform \"East\" step(2)\n",
    "* Perform \"Pickup\" step(4)\n",
    "* Perform \"West\" step(3)\n",
    "* Perform \"South\" step(0) for 5 times\n",
    "* Perfomr \"Dropoff\" (5)\n",
    "* Perform \"West\" step(3) for 4 times\n",
    "* Perfomr \"Dropoff\" (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "[1, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[44m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| : : : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(0)\n",
    "for p in env.decode(env.s): print (p)\n",
    "env.render()\n",
    "print (\"reward: \" + str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "[1, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[44m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| : : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "  (Deploy)\n",
      "reward: 40\n",
      "done: False\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "for p in env.decode(env.s): print (p)\n",
    "env.render()\n",
    "print (\"reward: \" + str(reward))\n",
    "print (\"done: \" + str(done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "[1, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[44m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| : : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "  (East)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(2)\n",
    "for p in env.decode(env.s): print (p)\n",
    "env.render()\n",
    "print (\"reward: \" + str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "[1, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[44m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "  (Deploy)\n",
      "reward: 50\n",
      "done: True\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "for p in env.decode(env.s): print (p)\n",
    "env.render()\n",
    "print (\"reward: \" + str(reward))\n",
    "print (\"done: \" + str(done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "You have probably figured out the rewards:\n",
    "* Perform any movements: -1\n",
    "* Pick up or drop off at the wrong position: -10\n",
    "* Drop off the passenger at the right position: 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Agent: \n",
    "\n",
    "We will use the funciton env.action_space.sample(); you could run the following cell for a few times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print (env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good does behaving completely random do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 5 Steps with a total reward of -203\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "done = None\n",
    "counter = 0\n",
    "g = 0\n",
    "reward = None\n",
    "while done != True:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    counter += 1\n",
    "    g += reward\n",
    "print(\"Solved in {} Steps with a total reward of {}\".format(counter,g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may luck out and solve the environment fairly quickly, but on average, a completely random policy will solve this environment in about ???? steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent with Basic Reinforcement Learning: Q-Learning\n",
    "\n",
    "In order to maximize our reward, we will have to have the algorithm remember its actions and their associated rewards. Here, the algorithm’s memory is going to be a Q action value table.\n",
    "\n",
    "To manage this Q table, we will use a NumPy array. The size of this table will be the number of states (2520) by the number of possible actions (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "+-----------------+\n",
      "|\u001b[44m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m: : : : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m| | | | | | |\n",
      "| : | | | : : : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "\n",
      "Initial State = 0\n",
      "Step: 0, Action: 0, Reward: -1, Q[0] \t[-0.05  0.    0.    0.    0.  ]\n",
      "Step: 1, Action: 0, Reward: -1, Q[142884] \t[-0.05  0.    0.    0.    0.  ]\n",
      "Step: 2, Action: 0, Reward: -50, Q[285768] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 3, Action: 0, Reward: -50, Q[428652] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 4, Action: 0, Reward: -50, Q[571536] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 5, Action: 0, Reward: -50, Q[714420] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 6, Action: 0, Reward: -50, Q[857304] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 7, Action: 0, Reward: -50, Q[1000188] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 8, Action: 0, Reward: -50, Q[1143072] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 9, Action: 0, Reward: -50, Q[1285956] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 10, Action: 0, Reward: -50, Q[1428840] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 11, Action: 0, Reward: -50, Q[1571724] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 12, Action: 0, Reward: -50, Q[1714608] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 13, Action: 0, Reward: -50, Q[1857492] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 14, Action: 1, Reward: -50, Q[1857492] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 15, Action: 1, Reward: -50, Q[1714608] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 16, Action: 1, Reward: -50, Q[1571724] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 17, Action: 1, Reward: -50, Q[1428840] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 18, Action: 1, Reward: -50, Q[1285956] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 19, Action: 1, Reward: -50, Q[1143072] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 20, Action: 1, Reward: -50, Q[1000188] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 21, Action: 1, Reward: -50, Q[857304] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 22, Action: 1, Reward: -50, Q[714420] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 23, Action: 1, Reward: -50, Q[571536] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 24, Action: 1, Reward: -1, Q[428652] \t[-2.5  -0.05  0.    0.    0.  ]\n",
      "Step: 25, Action: 1, Reward: -1, Q[285768] \t[-2.5  -0.05  0.    0.    0.  ]\n",
      "Step: 26, Action: 1, Reward: -1, Q[142884] \t[-0.05 -0.05  0.    0.    0.  ]\n",
      "Step: 27, Action: 1, Reward: -1, Q[0] \t[-0.05 -0.05  0.    0.    0.  ]\n",
      "Step: 28, Action: 2, Reward: -1, Q[0] \t[-0.05 -0.05 -0.05  0.    0.  ]\n",
      "Step: 29, Action: 3, Reward: -1, Q[0] \t[-0.05 -0.05 -0.05 -0.05  0.  ]\n",
      "Step: 30, Action: 4, Reward: -100, Q[0] \t[-0.05 -0.05 -0.05 -0.05 -5.  ]\n",
      "Step: 31, Action: 0, Reward: -1, Q[2000376] \t[-0.05  0.    0.    0.    0.  ]\n",
      "Step: 32, Action: 0, Reward: -1, Q[2143260] \t[-0.05  0.    0.    0.    0.  ]\n",
      "Step: 33, Action: 0, Reward: -50, Q[2286144] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 34, Action: 0, Reward: -50, Q[2429028] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 35, Action: 0, Reward: -50, Q[2571912] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 36, Action: 0, Reward: -50, Q[2714796] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 37, Action: 0, Reward: -50, Q[2857680] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 38, Action: 0, Reward: -50, Q[3000564] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 39, Action: 0, Reward: -50, Q[3143448] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 40, Action: 0, Reward: -50, Q[3286332] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 41, Action: 0, Reward: -50, Q[3429216] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 42, Action: 0, Reward: -50, Q[3572100] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 43, Action: 0, Reward: -50, Q[3714984] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 44, Action: 0, Reward: -50, Q[3857868] \t[-2.5  0.   0.   0.   0. ]\n",
      "Step: 45, Action: 1, Reward: -50, Q[3857868] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 46, Action: 1, Reward: -50, Q[3714984] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 47, Action: 1, Reward: -50, Q[3572100] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 48, Action: 1, Reward: -50, Q[3429216] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 49, Action: 1, Reward: -50, Q[3286332] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 50, Action: 1, Reward: -50, Q[3143448] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 51, Action: 1, Reward: -50, Q[3000564] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 52, Action: 1, Reward: -50, Q[2857680] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 53, Action: 1, Reward: -50, Q[2714796] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 54, Action: 1, Reward: -50, Q[2571912] \t[-2.5 -2.5  0.   0.   0. ]\n",
      "Step: 55, Action: 1, Reward: -1, Q[2429028] \t[-2.5  -0.05  0.    0.    0.  ]\n",
      "Step: 56, Action: 1, Reward: -1, Q[2286144] \t[-2.5  -0.05  0.    0.    0.  ]\n",
      "Step: 57, Action: 1, Reward: -1, Q[2143260] \t[-0.05 -0.05  0.    0.    0.  ]\n",
      "Step: 58, Action: 1, Reward: -1, Q[2000376] \t[-0.05 -0.05  0.    0.    0.  ]\n",
      "Step: 59, Action: 2, Reward: -1, Q[2000376] \t[-0.05 -0.05 -0.05  0.    0.  ]\n",
      "Step: 60, Action: 3, Reward: -1, Q[2000376] \t[-0.05 -0.05 -0.05 -0.05  0.  ]\n",
      "Step: 61, Action: 4, Reward: -100, Q[2000376] \t[-0.05 -0.05 -0.05 -0.05 -5.  ]\n",
      "Final State = 2000376\n",
      "Solved in 62 Steps with a total reward of -2416\n",
      "[-0.05 -0.05 -0.05 -0.05 -5.  ]\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = np.zeros([n_states, n_actions])\n",
    "\n",
    "episodes = 1\n",
    "G = 0\n",
    "counter = 0\n",
    "alpha = 0.05\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    firstState = state\n",
    "    print(\"Initial State = {}\".format(state))\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state])  #1\n",
    "        state2, reward, done, info = env.step(action) #2\n",
    "        Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) #3\n",
    "        \n",
    "        if counter < 100:\n",
    "            print(\"Step: {}, Action: {}, Reward: {}, Q[{}] \\t{}\".format(counter, action, reward, state, Q[state]))\n",
    "\n",
    "        counter += 1\n",
    "        G += reward\n",
    "        state = state2\n",
    "        \n",
    "finalState = state\n",
    "print(\"Final State = {}\".format(finalState))\n",
    "print(\"Solved in {} Steps with a total reward of {}\".format(counter, G))\n",
    "\n",
    "print (Q[finalState])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First (#1): The agent starts by choosing an action with the highest Q value for the current state using argmax. Argmax will return the index/action with the highest value for that state. Initially, our Q table will be all zeros. But, after every step, the Q values for state-action pairs will be updated.\n",
    "\n",
    "Second (#2): The agent then takes action and we store the future state as state2 (S t+1). This will allow the agent to compare the previous state to the new state.\n",
    "\n",
    "Third (#3): We update the state-action pair (St , At) for Q using the reward, and the max Q value for state2 (S t+1). This update is done using the action value formula (based upon the Bellman equation) and allows state-action pairs to be updated in a recursive fashion (based on future values). See the following Figure for the value iteration update.\n",
    "\n",
    "<img src=\"images/qlearn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run over multiple episodes so that we can converge on a optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 Total Reward: -3834\n",
      "Episode 10 Total Reward: -862\n",
      "Episode 15 Total Reward: 58\n",
      "Episode 20 Total Reward: 58\n",
      "Episode 25 Total Reward: -2021\n",
      "Episode 30 Total Reward: -866\n",
      "Episode 35 Total Reward: 58\n",
      "Episode 40 Total Reward: 98\n",
      "Episode 45 Total Reward: 98\n",
      "Episode 50 Total Reward: 98\n",
      "Episode 55 Total Reward: 78\n",
      "Episode 60 Total Reward: 78\n",
      "Episode 65 Total Reward: 98\n",
      "Episode 70 Total Reward: 78\n",
      "Episode 75 Total Reward: 98\n",
      "Episode 80 Total Reward: 58\n",
      "Episode 85 Total Reward: 98\n",
      "Episode 90 Total Reward: 98\n",
      "Episode 95 Total Reward: 58\n",
      "Episode 100 Total Reward: 98\n",
      "Episode 105 Total Reward: 58\n",
      "Episode 110 Total Reward: 98\n",
      "Episode 115 Total Reward: 98\n",
      "Episode 120 Total Reward: 58\n",
      "Episode 125 Total Reward: 98\n",
      "Episode 130 Total Reward: 98\n",
      "Episode 135 Total Reward: 98\n",
      "Episode 140 Total Reward: 98\n",
      "Episode 145 Total Reward: 58\n",
      "Episode 150 Total Reward: 58\n",
      "Episode 155 Total Reward: 58\n",
      "Episode 160 Total Reward: 98\n",
      "Episode 165 Total Reward: 78\n",
      "Episode 170 Total Reward: 58\n",
      "Episode 175 Total Reward: 98\n",
      "Episode 180 Total Reward: 78\n",
      "Episode 185 Total Reward: 78\n",
      "Episode 190 Total Reward: 58\n",
      "Episode 195 Total Reward: 98\n",
      "Episode 200 Total Reward: 98\n",
      "Episode 205 Total Reward: 58\n",
      "Episode 210 Total Reward: 98\n",
      "Episode 215 Total Reward: 98\n",
      "Episode 220 Total Reward: 98\n",
      "Episode 225 Total Reward: 58\n",
      "Episode 230 Total Reward: 78\n",
      "Episode 235 Total Reward: 98\n",
      "Episode 240 Total Reward: 58\n",
      "Episode 245 Total Reward: 98\n",
      "Episode 250 Total Reward: 78\n",
      "Episode 255 Total Reward: 58\n",
      "Episode 260 Total Reward: 58\n",
      "Episode 265 Total Reward: 78\n",
      "Episode 270 Total Reward: 78\n",
      "Episode 275 Total Reward: 98\n",
      "Episode 280 Total Reward: 78\n",
      "Episode 285 Total Reward: 98\n",
      "Episode 290 Total Reward: 78\n",
      "Episode 295 Total Reward: 78\n",
      "Episode 300 Total Reward: 78\n",
      "Episode 305 Total Reward: 78\n",
      "Episode 310 Total Reward: 78\n",
      "Episode 315 Total Reward: 78\n",
      "Episode 320 Total Reward: 78\n",
      "Episode 325 Total Reward: 78\n",
      "Episode 330 Total Reward: 98\n",
      "Episode 335 Total Reward: 98\n",
      "Episode 340 Total Reward: 98\n",
      "Episode 345 Total Reward: 78\n",
      "Episode 350 Total Reward: 98\n",
      "Episode 355 Total Reward: 78\n",
      "Episode 360 Total Reward: 58\n",
      "Episode 365 Total Reward: 98\n",
      "Episode 370 Total Reward: 98\n",
      "Episode 375 Total Reward: 78\n",
      "Episode 380 Total Reward: 98\n",
      "Episode 385 Total Reward: 98\n",
      "Episode 390 Total Reward: 78\n",
      "Episode 395 Total Reward: 58\n",
      "Episode 400 Total Reward: 58\n",
      "Episode 405 Total Reward: 98\n",
      "Episode 410 Total Reward: 98\n",
      "Episode 415 Total Reward: 98\n",
      "Episode 420 Total Reward: 58\n",
      "Episode 425 Total Reward: 98\n",
      "Episode 430 Total Reward: 98\n",
      "Episode 435 Total Reward: 58\n",
      "Episode 440 Total Reward: 58\n",
      "Episode 445 Total Reward: 98\n",
      "Episode 450 Total Reward: 98\n",
      "Episode 455 Total Reward: 98\n",
      "Episode 460 Total Reward: 58\n",
      "Episode 465 Total Reward: 58\n",
      "Episode 470 Total Reward: 98\n",
      "Episode 475 Total Reward: 78\n",
      "Episode 480 Total Reward: 58\n",
      "Episode 485 Total Reward: 58\n",
      "Episode 490 Total Reward: 98\n",
      "Episode 495 Total Reward: 58\n",
      "Episode 500 Total Reward: 78\n"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "rewardTracker = []\n",
    "\n",
    "G = 0\n",
    "alpha = 0.05\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state]) \n",
    "        state2, reward, done, info = env.step(action) \n",
    "        Q[state,action] += alpha * ((reward + (np.max(Q[state2]))  - Q[state,action]))\n",
    "        G += reward\n",
    "        state = state2\n",
    "    \n",
    "    if episode % 5 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode,G))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have learned the optimal Q Values we have developed a optimal policy and have no need to train the agent anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[44m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "[2, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[44m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "  (Deploy)\n",
      "[2, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[44m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : | | |\n",
      "| | | | | | | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "  (South)\n",
      "[2, 0]\n",
      "+-----------------+\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m: : : : |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[44m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "|\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m:\u001b[43m_\u001b[0m: : | | |\n",
      "|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m|\u001b[43m_\u001b[0m| | | | |\n",
      "| | | : : | | : : |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : | | |\n",
      "| | | | | | | | | |\n",
      "| : : : : : : : : |\n",
      "+-----------------+\n",
      "  (Deploy)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "done = None\n",
    "\n",
    "while done != True:\n",
    "    # We simply take the action with the highest Q Value\n",
    "    action = np.argmax(Q[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
